{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At4DQjFKMBU8",
        "outputId": "83b616ac-b8cb-4e53-ae82-79e165343bb9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = pd.read_csv('/mnt/c/PROJECTS/PRO/training_set.csv', index_col=0)\n",
        "y = X['label']\n",
        "y = y.fillna('None')  \n",
        "X = X.drop(columns=['label']) \n",
        "X = X.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_val = pd.read_csv('/mnt/c/PROJECTS/PRO/validation_set.csv', index_col=0)\n",
        "y_val = X_val['label']\n",
        "y_val = y_val.fillna('None') \n",
        "X_val = X_val.drop(columns=['label']) \n",
        "X_val = X_val.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = pd.read_csv('/mnt/c/PROJECTS/PRO/test_set.csv', index_col=0)\n",
        "y_test = X_test['label']\n",
        "y_test = y_test.fillna('None') \n",
        "X_test = X_test.drop(columns=['label']) \n",
        "X_test = X_test.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Forest without tuning and checking feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5aJDV-8d4FKp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "topics            14477\n",
            "None              14477\n",
            "negative          14477\n",
            "affirmative       14477\n",
            "relative          14477\n",
            "yn_question       14477\n",
            "conditional       14477\n",
            "doubt_question    14477\n",
            "emphasis          14477\n",
            "wh_question       14477\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X, y)\n",
        "\n",
        "print(y_train.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          Feature  Importance\n",
            "239              79z_face_contour    0.002183\n",
            "91              30y_right_eyebrow    0.001718\n",
            "14                    4z_left_eye    0.001396\n",
            "185                     61z_mouth    0.001324\n",
            "48               16x_left_eyebrow    0.001288\n",
            "173                     57z_mouth    0.001253\n",
            "168                     56x_mouth    0.001181\n",
            "55               18y_left_eyebrow    0.001181\n",
            "158                     52z_mouth    0.001110\n",
            "248              82z_face_contour    0.001110\n",
            "182                     60z_mouth    0.001074\n",
            "17                    5z_left_eye    0.001038\n",
            "16                    5y_left_eye    0.001038\n",
            "89              29z_right_eyebrow    0.000966\n",
            "18                    6x_left_eye    0.000931\n",
            "216              72x_face_contour    0.000931\n",
            "109                      36y_nose    0.000895\n",
            "166                     55y_mouth    0.000859\n",
            "285  95x_line_above_right_eyebrow    0.000859\n",
            "8                     2z_left_eye    0.000823\n",
            "103             34y_right_eyebrow    0.000823\n",
            "202                     67y_mouth    0.000823\n",
            "75               25x_left_eyebrow    0.000823\n",
            "298  99y_line_above_right_eyebrow    0.000787\n",
            "43                  14y_right_eye    0.000787\n",
            "60               20x_left_eyebrow    0.000787\n",
            "159                     53x_mouth    0.000787\n",
            "19                    6y_left_eye    0.000716\n",
            "227              75z_face_contour    0.000716\n",
            "242              80z_face_contour    0.000680\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': perm_importance.importances_mean\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(importances.head(30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.78      0.92      0.84        83\n",
            "   conditional       0.82      0.88      0.85        95\n",
            "doubt_question       0.94      0.96      0.95       148\n",
            "      emphasis       0.91      0.93      0.92        87\n",
            "      negative       0.89      0.94      0.91       125\n",
            "      relative       0.91      0.92      0.91       147\n",
            "        topics       0.87      0.89      0.88        90\n",
            "   wh_question       0.88      0.92      0.90       114\n",
            "   yn_question       0.82      0.95      0.88       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.88      0.92      0.90      2794\n",
            "  weighted avg       0.93      0.93      0.93      2794\n",
            "\n",
            "Accuracy: 0.93\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(report)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_added = X.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_distances(points):\n",
        "    distances = []\n",
        "    for i in range(len(points) - 1):\n",
        "        distances.append(np.sqrt((points[i+1][0] - points[i][0])**2 +\n",
        "                                 (points[i+1][1] - points[i][1])**2 +\n",
        "                                 (points[i+1][2] - points[i][2])**2))\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_total_distance(df, lower_bound, upper_bound, suffix, axis_labels=['x', 'y', 'z']):\n",
        "    total_distances = []  \n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        points = [\n",
        "            tuple(row[f\"{i}{axis}{suffix}\"] for axis in axis_labels)\n",
        "            for i in range(lower_bound, upper_bound + 1)\n",
        "        ]\n",
        "\n",
        "        total_distance = sum(\n",
        "            np.sqrt(sum((p2 - p1) ** 2 for p1, p2 in zip(points[i], points[i + 1])))\n",
        "            for i in range(len(points) - 1)\n",
        "        )\n",
        "        total_distances.append(total_distance)\n",
        "\n",
        "    return total_distances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_distance_between_two_points(df, point1, suffix1, point2, suffix2, axis_labels=[\"x\", \"y\", \"z\"]):\n",
        "    distances = []\n",
        "    for _, row in df.iterrows():\n",
        "        coords1 = [row[f\"{point1}{axis}{suffix1}\"] for axis in axis_labels]\n",
        "        coords2 = [row[f\"{point2}{axis}{suffix2}\"] for axis in axis_labels]\n",
        "\n",
        "        distance = np.sqrt(sum((c2 - c1) ** 2 for c1, c2 in zip(coords1, coords2)))\n",
        "        distances.append(distance)\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_added[\"left_eyebrow_length\"] = calculate_total_distance(X, 16, 20, \"_left_eyebrow\")\n",
        "\n",
        "X_added[\"right_eyebrow_length\"] = calculate_total_distance(X, 26, 30, \"_right_eyebrow\")\n",
        "\n",
        "X_added[\"mouth_length\"] = calculate_distance_between_two_points(X, 48, \"_mouth\", 54, \"_mouth\")\n",
        "\n",
        "X_added[\"left_eye_width\"] = calculate_distance_between_two_points(X, 4, \"_left_eye\", 0, \"_left_eye\")\n",
        "\n",
        "X_added[\"right_eye_width\"] = calculate_distance_between_two_points(X, 12, \"_right_eye\", 8, \"_right_eye\")\n",
        "\n",
        "X_added[\"nose_to_left_eye\"] = calculate_distance_between_two_points(X, 89, \"_nose_tip\", 0, \"_left_eye\")\n",
        "\n",
        "X_added[\"nose_to_right_eye\"] = calculate_distance_between_two_points(X, 89, \"_nose_tip\", 8, \"_right_eye\")\n",
        "\n",
        "nose_to_51 = calculate_distance_between_two_points(X, 89, \"_nose_tip\", 51, \"_mouth\")\n",
        "nose_to_57 = calculate_distance_between_two_points(X, 89, \"_nose_tip\", 57, \"_mouth\")\n",
        "X_added[\"nose_to_mouth_center\"] = [(d1 + d2) / 2 for d1, d2 in zip(nose_to_51, nose_to_57)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_val_added = X_val.copy()\n",
        "X_val_added[\"left_eyebrow_length\"] = calculate_total_distance(X_val, 16, 20, \"_left_eyebrow\")\n",
        "X_val_added[\"right_eyebrow_length\"] = calculate_total_distance(X_val, 26, 30, \"_right_eyebrow\")\n",
        "X_val_added[\"mouth_length\"] = calculate_distance_between_two_points(X_val, 48, \"_mouth\", 54, \"_mouth\")\n",
        "X_val_added[\"left_eye_width\"] = calculate_distance_between_two_points(X_val, 4, \"_left_eye\", 0, \"_left_eye\")\n",
        "X_val_added[\"right_eye_width\"] = calculate_distance_between_two_points(X_val, 12, \"_right_eye\", 8, \"_right_eye\")\n",
        "X_val_added[\"nose_to_left_eye\"] = calculate_distance_between_two_points(X_val, 89, \"_nose_tip\", 0, \"_left_eye\")\n",
        "X_val_added[\"nose_to_right_eye\"] = calculate_distance_between_two_points(X_val, 89, \"_nose_tip\", 8, \"_right_eye\")\n",
        "\n",
        "nose_to_51_val = calculate_distance_between_two_points(X_val, 89, \"_nose_tip\", 51, \"_mouth\")\n",
        "nose_to_57_val = calculate_distance_between_two_points(X_val, 89, \"_nose_tip\", 57, \"_mouth\")\n",
        "X_val_added[\"nose_to_mouth_center\"] = [(d1 + d2) / 2 for d1, d2 in zip(nose_to_51_val, nose_to_57_val)]\n",
        "\n",
        "X_test_added = X_test.copy()\n",
        "X_test_added[\"left_eyebrow_length\"] = calculate_total_distance(X_test, 16, 20, \"_left_eyebrow\")\n",
        "X_test_added[\"right_eyebrow_length\"] = calculate_total_distance(X_test, 26, 30, \"_right_eyebrow\")\n",
        "X_test_added[\"mouth_length\"] = calculate_distance_between_two_points(X_test, 48, \"_mouth\", 54, \"_mouth\")\n",
        "X_test_added[\"left_eye_width\"] = calculate_distance_between_two_points(X_test, 4, \"_left_eye\", 0, \"_left_eye\")\n",
        "X_test_added[\"right_eye_width\"] = calculate_distance_between_two_points(X_test, 12, \"_right_eye\", 8, \"_right_eye\")\n",
        "X_test_added[\"nose_to_left_eye\"] = calculate_distance_between_two_points(X_test, 89, \"_nose_tip\", 0, \"_left_eye\")\n",
        "X_test_added[\"nose_to_right_eye\"] = calculate_distance_between_two_points(X_test, 89, \"_nose_tip\", 8, \"_right_eye\")\n",
        "\n",
        "nose_to_51_test = calculate_distance_between_two_points(X_test, 89, \"_nose_tip\", 51, \"_mouth\")\n",
        "nose_to_57_test = calculate_distance_between_two_points(X_test, 89, \"_nose_tip\", 57, \"_mouth\")\n",
        "X_test_added[\"nose_to_mouth_center\"] = [(d1 + d2) / 2 for d1, d2 in zip(nose_to_51_test, nose_to_57_test)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_angle_between_three_points(df, point1, suffix1, point2, suffix2, point3, suffix3, axis_labels=[\"x\", \"y\", \"z\"]):\n",
        "    angles = []\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        coords1 = np.array([row[f\"{point1}{axis}{suffix1}\"] for axis in axis_labels])\n",
        "        coords2 = np.array([row[f\"{point2}{axis}{suffix2}\"] for axis in axis_labels])\n",
        "        coords3 = np.array([row[f\"{point3}{axis}{suffix3}\"] for axis in axis_labels])\n",
        "\n",
        "        vector1 = coords1 - coords2  \n",
        "        vector2 = coords3 - coords2  \n",
        "\n",
        "        dot_product = np.dot(vector1, vector2)\n",
        "        norm1 = np.linalg.norm(vector1)\n",
        "        norm2 = np.linalg.norm(vector2)\n",
        "\n",
        "        angle = np.arccos(dot_product / (norm1 * norm2))\n",
        "        angles.append(np.degrees(angle))  \n",
        "\n",
        "    return angles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_added[\"left_eyebrow_angle\"] = calculate_angle_between_three_points(\n",
        "    X, 16, \"_left_eyebrow\", 18, \"_left_eyebrow\", 20, \"_left_eyebrow\"\n",
        ")\n",
        "\n",
        "X_added[\"right_eyebrow_angle\"] = calculate_angle_between_three_points(\n",
        "    X, 26, \"_right_eyebrow\", 28, \"_right_eyebrow\", 30, \"_right_eyebrow\"\n",
        ")\n",
        "X_added[\"nose_to_eyes_angle\"] = calculate_angle_between_three_points(\n",
        "    X, 0, \"_left_eye\", 89, \"_nose_tip\", 8, \"_right_eye\"\n",
        ")\n",
        "\n",
        "X_added[\"mouth_angle\"] = calculate_angle_between_three_points(\n",
        "    X, 48, \"_mouth\", 51, \"_mouth\", 54, \"_mouth\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "X_val_added[\"left_eyebrow_angle\"] = calculate_angle_between_three_points(\n",
        "    X_val, 16, \"_left_eyebrow\", 18, \"_left_eyebrow\", 20, \"_left_eyebrow\"\n",
        ")\n",
        "\n",
        "X_val_added[\"right_eyebrow_angle\"] = calculate_angle_between_three_points(\n",
        "    X_val, 26, \"_right_eyebrow\", 28, \"_right_eyebrow\", 30, \"_right_eyebrow\"\n",
        ")\n",
        "\n",
        "X_val_added[\"nose_to_eyes_angle\"] = calculate_angle_between_three_points(\n",
        "    X_val, 0, \"_left_eye\", 89, \"_nose_tip\", 8, \"_right_eye\"\n",
        ")\n",
        "\n",
        "X_val_added[\"mouth_angle\"] = calculate_angle_between_three_points(\n",
        "    X_val, 48, \"_mouth\", 51, \"_mouth\", 54, \"_mouth\"\n",
        ")\n",
        "\n",
        "X_test_added[\"left_eyebrow_angle\"] = calculate_angle_between_three_points(\n",
        "    X_test, 16, \"_left_eyebrow\", 18, \"_left_eyebrow\", 20, \"_left_eyebrow\"\n",
        ")\n",
        "\n",
        "X_test_added[\"right_eyebrow_angle\"] = calculate_angle_between_three_points(\n",
        "    X_test, 26, \"_right_eyebrow\", 28, \"_right_eyebrow\", 30, \"_right_eyebrow\"\n",
        ")\n",
        "\n",
        "X_test_added[\"nose_to_eyes_angle\"] = calculate_angle_between_three_points(\n",
        "    X_test, 0, \"_left_eye\", 89, \"_nose_tip\", 8, \"_right_eye\"\n",
        ")\n",
        "\n",
        "X_test_added[\"mouth_angle\"] = calculate_angle_between_three_points(\n",
        "    X_test, 48, \"_mouth\", 51, \"_mouth\", 54, \"_mouth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       0x_left_eye  0y_left_eye  0z_left_eye  1x_left_eye  1y_left_eye  1z_left_eye  2x_left_eye  2y_left_eye  2z_left_eye  3x_left_eye  3y_left_eye  3z_left_eye  4x_left_eye  4y_left_eye  4z_left_eye  5x_left_eye  5y_left_eye  5z_left_eye  6x_left_eye  6y_left_eye  6z_left_eye  7x_left_eye  7y_left_eye  7z_left_eye  8x_right_eye  8y_right_eye  8z_right_eye  9x_right_eye  9y_right_eye  9z_right_eye  10x_right_eye  10y_right_eye  10z_right_eye  11x_right_eye  11y_right_eye  11z_right_eye  12x_right_eye  12y_right_eye  12z_right_eye  13x_right_eye  13y_right_eye  13z_right_eye  14x_right_eye  14y_right_eye  14z_right_eye  15x_right_eye  15y_right_eye  15z_right_eye  16x_left_eyebrow  16y_left_eyebrow  16z_left_eyebrow  17x_left_eyebrow  17y_left_eyebrow  17z_left_eyebrow  18x_left_eyebrow  18y_left_eyebrow  18z_left_eyebrow  19x_left_eyebrow  19y_left_eyebrow  19z_left_eyebrow  20x_left_eyebrow  20y_left_eyebrow  20z_left_eyebrow  21x_left_eyebrow  21y_left_eyebrow  21z_left_eyebrow  \\\n",
            "3366       311.369      231.599       1200.0      309.390      229.100       1200.0      306.305      228.006       1195.0      303.362      228.332       1200.0      301.531      229.988       1200.0      303.483      231.145       1204.0      306.162      231.692       1200.0      308.977      231.841       1195.0       325.741       233.198        1212.0       328.019       231.132        1212.0        331.402        230.702         1216.0        334.878        231.605         1212.0        337.453        233.655         1212.0        334.726        234.223         1212.0        331.750        234.252         1212.0        328.573        233.865         1208.0           312.267           221.441            1212.0           309.368           219.247            1212.0           305.895           217.964            1229.0           302.823           218.182            1234.0           300.463           219.764            1234.0           300.229           222.134            1238.0   \n",
            "6885       281.507      202.836       1221.0      278.776      201.712       1221.0      275.608      201.596       1221.0      272.633      202.302       1225.0      270.383      203.685       1229.0      272.882      204.378       1225.0      275.903      204.392       1221.0      278.819      203.824       1216.0       298.419       201.947        1229.0       301.235       200.666        1234.0        304.680        200.212         1238.0        307.893        200.607         1243.0        310.712        201.727         1252.0        307.991        202.762         1243.0        304.744        203.105         1238.0        301.392        202.806         1234.0           280.942           193.242            1216.0           276.198           192.296            1212.0           271.044           192.551            1234.0           267.781           194.671            1238.0           265.886           197.500            1238.0           266.694           199.127            1238.0   \n",
            "27397      329.960      223.782       1256.0      327.808      222.649       1265.0      324.948      222.352       1270.0      322.170      222.802       1270.0      319.961      223.911       1275.0      322.211      224.706       1270.0      324.902      224.931       1265.0      327.640      224.663       1261.0       345.973       223.489        1275.0       348.029       222.153        1275.0        350.843        221.705         1275.0        353.604        221.986         1284.0        355.901        223.023         1284.0        353.742        223.937         1275.0        351.046        224.304         1275.0        348.229        224.152         1275.0           330.962           216.699            1247.0           327.591           215.680            1256.0           324.166           215.246            1275.0           320.629           215.439            1280.0           317.614           216.415            1289.0           317.472           218.579            1289.0   \n",
            "19636      302.328      200.100       1225.0      299.703      198.183       1225.0      296.323      197.526       1225.0      292.970      198.416       1234.0      290.538      200.632       1238.0      293.201      201.354       1229.0      296.407      201.500       1225.0      299.633      201.018       1225.0       319.524       200.136        1234.0       322.113       198.345        1238.0        325.602        197.714         1238.0        329.016        198.632         1247.0        331.346        200.819         1256.0        328.767        201.519         1247.0        325.473        201.572         1238.0        322.232        201.060         1234.0           303.457           189.175            1216.0           297.926           188.159            1234.0           291.872           188.566            1243.0           287.863           190.984            1243.0           285.359           194.237            1243.0           286.298           195.571            1243.0   \n",
            "13734      290.403      237.834       1247.0      287.585      235.963       1252.0      284.068      235.497       1256.0      280.690      236.153       1265.0      278.316      237.842       1270.0      280.897      239.117       1265.0      284.171      239.404       1261.0      287.525      238.854       1252.0       307.857       236.608        1256.0       310.424       234.436        1261.0        313.909        233.560         1261.0        317.491        233.759         1261.0        320.435        235.033         1265.0        317.735        236.579         1265.0        314.479        237.272         1261.0        311.025        237.239         1261.0           289.229           226.353            1247.0           284.727           224.745            1270.0           279.585           224.159            1280.0           275.915           225.690            1289.0           273.598           228.335            1289.0           274.160           230.576            1289.0   \n",
            "\n",
            "       22x_left_eyebrow  22y_left_eyebrow  22z_left_eyebrow  23x_left_eyebrow  23y_left_eyebrow  23z_left_eyebrow  24x_left_eyebrow  24y_left_eyebrow  24z_left_eyebrow  25x_left_eyebrow  25y_left_eyebrow  25z_left_eyebrow  26x_right_eyebrow  26y_right_eyebrow  26z_right_eyebrow  27x_right_eyebrow  27y_right_eyebrow  27z_right_eyebrow  28x_right_eyebrow  28y_right_eyebrow  28z_right_eyebrow  29x_right_eyebrow  29y_right_eyebrow  29z_right_eyebrow  30x_right_eyebrow  30y_right_eyebrow  30z_right_eyebrow  31x_right_eyebrow  31y_right_eyebrow  31z_right_eyebrow  32x_right_eyebrow  32y_right_eyebrow  32z_right_eyebrow  33x_right_eyebrow  33y_right_eyebrow  33z_right_eyebrow  34x_right_eyebrow  34y_right_eyebrow  34z_right_eyebrow  35x_right_eyebrow  35y_right_eyebrow  35z_right_eyebrow  36x_nose  36y_nose  36z_nose  37x_nose  37y_nose     37z_nose  38x_nose  38y_nose  38z_nose  39x_nose  39y_nose  39z_nose  40x_nose  40y_nose  40z_nose  41x_nose  41y_nose  41z_nose  42x_nose  42y_nose  \\\n",
            "3366            302.858           221.477            1238.0           305.670           221.635            1216.0           308.903           222.606            1204.0           311.977           224.065            1204.0            324.216            221.863             1221.0            328.846            220.561             1216.0            334.187            220.266             1221.0            338.506            221.712             1225.0            341.725            223.931             1221.0            341.413            226.496             1225.0            337.639            225.075             1221.0            333.419            224.249             1221.0            328.666            224.356             1216.0            324.133            224.974             1216.0   314.203   232.218    1208.0   312.173   238.522  1205.333333   309.552   244.426    1208.0   306.554   249.741    1204.0   307.438   254.137    1204.0   311.865   255.186    1204.0   315.343   255.540   \n",
            "6885            269.001           197.073            1238.0           272.008           195.824            1234.0           276.303           195.647            1221.0           280.818           196.105            1221.0            296.729            192.371             1238.0            302.104            190.895             1247.0            308.259            190.422             1256.0            312.668            192.301             1261.0            315.628            194.915             1270.0            315.030            196.723             1270.0            311.586            194.828             1256.0            307.691            193.969             1247.0            302.415            194.255             1243.0            297.107            195.259             1234.0   285.852   202.030    1225.0   285.421   206.265  1221.000000   283.857   209.835    1216.0   281.136   212.825    1216.0   281.528   216.674    1216.0   286.423   216.868    1216.0   290.480   216.763   \n",
            "27397           320.667           218.040            1275.0           324.124           217.874            1270.0           327.384           218.100            1256.0           330.926           218.708            1247.0            344.698            216.335             1289.0            347.894            215.163             1289.0            351.453            214.502             1294.0            355.044            214.450             1294.0            358.149            215.191             1299.0            358.289            217.376             1294.0            355.018            217.041             1294.0            351.595            217.125             1289.0            348.174            217.580             1289.0            344.799            218.347             1275.0   334.650   223.937    1256.0   333.931   227.788  1252.000000   332.524   231.630    1247.0   330.164   235.294    1247.0   331.360   238.355    1247.0   336.230   238.461    1252.0   340.164   238.400   \n",
            "19636           289.253           193.133            1243.0           292.796           191.697            1238.0           297.934           191.562            1225.0           303.136           192.209            1221.0            318.244            189.084             1247.0            323.832            188.187             1247.0            330.220            188.689             1261.0            334.411            191.221             1270.0            336.814            194.322             1275.0            335.799            195.753             1270.0            332.862            193.292             1261.0            329.154            191.828             1256.0            323.752            191.598             1243.0            318.373            192.150             1243.0   307.103   199.641    1225.0   306.294   203.530  1225.000000   304.422   206.900    1225.0   301.589   210.030    1225.0   302.571   213.891    1221.0   308.076   213.143    1225.0   311.893   213.153   \n",
            "13734           276.912           228.890            1289.0           280.280           228.097            1280.0           284.699           228.392            1256.0           289.185           229.291            1243.0            305.701            224.770             1270.0            310.513            222.655             1270.0            316.094            221.311             1275.0            320.628            222.298             1270.0            323.957            224.357             1280.0            323.903            226.747             1275.0            320.107            225.657             1270.0            315.938            225.470             1270.0            311.038            226.385             1270.0            306.146            227.895             1265.0   294.778   237.713    1247.0   294.816   244.067  1247.000000   293.687   250.059    1243.0   290.951   255.035    1247.0   292.074   259.563    1243.0   297.509   260.079    1247.0   302.244   259.749   \n",
            "\n",
            "       42z_nose  43x_nose  43y_nose  43z_nose  44x_nose  44y_nose  44z_nose  45x_nose  45y_nose     45z_nose  46x_nose  46y_nose     46z_nose  47x_nose  47y_nose     47z_nose  48x_mouth  48y_mouth  48z_mouth  49x_mouth  49y_mouth  49z_mouth  50x_mouth  50y_mouth  50z_mouth  51x_mouth  51y_mouth  51z_mouth  52x_mouth  52y_mouth  52z_mouth  53x_mouth  53y_mouth  53z_mouth  54x_mouth  54y_mouth  54z_mouth  55x_mouth  55y_mouth  55z_mouth  56x_mouth  56y_mouth  56z_mouth  57x_mouth  57y_mouth  57z_mouth  58x_mouth  58y_mouth  58z_mouth  59x_mouth  59y_mouth  59z_mouth  60x_mouth  60y_mouth  60z_mouth  61x_mouth  61y_mouth  61z_mouth  62x_mouth  62y_mouth  62z_mouth  63x_mouth  63y_mouth  63z_mouth  64x_mouth  64y_mouth  64z_mouth  65x_mouth  65y_mouth  65z_mouth  66x_mouth  66y_mouth  66z_mouth  67x_mouth  67y_mouth  67z_mouth  68x_face_contour  68y_face_contour  68z_face_contour  69x_face_contour  69y_face_contour  69z_face_contour  70x_face_contour  70y_face_contour  70z_face_contour  \\\n",
            "3366     1204.0   322.466   256.057    1195.0   324.210   251.334    1200.0   321.481   245.738  1208.000000   320.465   239.642  1212.000000   320.461   232.896  1212.000000    306.063    266.952     1200.0    307.975    264.804     1204.0    310.733    263.509     1200.0    313.629    264.629     1204.0    316.805    264.221     1204.0    321.375    266.422     1200.0    325.697    269.124     1200.0    322.355    271.848     1200.0    318.544    273.334     1204.0    314.251    273.666     1200.0    310.609    272.604     1195.0    308.214    270.487     1200.0    308.131    267.471     1195.0    310.878    267.349     1200.0    314.249    267.662     1204.0    318.748    268.285     1204.0    323.190    269.286     1200.0    318.628    269.477     1204.0    314.318    269.477     1200.0    311.008    268.693     1195.0           296.590           230.006            1238.0           294.503           238.494            1234.0           293.742           246.774            1229.0   \n",
            "6885     1221.0   297.046   216.230    1225.0   297.856   212.546    1225.0   294.823   209.900  1225.000000   293.503   206.097  1221.000000   293.246   201.868  1225.000000    279.322    225.950     1212.0    282.022    223.099     1212.0    285.588    221.189     1212.0    289.020    221.685     1212.0    292.586    220.909     1221.0    297.223    222.437     1225.0    301.181    224.884     1238.0    298.065    228.015     1234.0    294.356    230.049     1229.0    289.669    230.820     1225.0    285.334    230.345     1221.0    281.954    228.767     1221.0    281.521    225.839     1212.0    285.040    224.466     1208.0    289.308    224.295     1212.0    294.147    224.050     1221.0    298.779    224.898     1229.0    294.335    225.918     1221.0    289.451    226.396     1216.0    284.911    226.271     1212.0           265.644           205.198            1234.0           265.902           211.548            1234.0           266.894           217.660            1234.0   \n",
            "27397    1238.0   344.963   238.082    1252.0   345.975   234.857    1247.0   343.457   231.349  1245.666667   342.033   227.577  1265.222222   341.241   223.778  1271.740741    328.877    247.570     1229.0    331.708    245.379     1234.0    335.014    243.908     1243.0    338.180    244.147     1243.0    341.116    243.729     1247.0    344.617    245.008     1238.0    347.770    247.082     1243.0    345.286    248.858     1238.0    342.389    250.018     1234.0    338.344    250.531     1234.0    334.239    250.156     1234.0    331.346    249.083     1229.0    331.125    247.451     1229.0    334.633    246.591     1234.0    338.314    246.345     1238.0    341.978    246.437     1238.0    345.612    247.106     1238.0    341.973    247.249     1238.0    338.294    247.408     1234.0    334.668    247.397     1234.0           310.095           224.321            1284.0           309.988           230.135            1275.0           310.505           235.925            1265.0   \n",
            "19636    1229.0   317.806   213.982    1238.0   319.041   210.168    1234.0   316.418   206.957  1234.000000   314.987   203.549  1229.000000   314.529   199.670  1229.000000    298.879    225.806     1225.0    302.032    221.864     1221.0    306.199    219.103     1221.0    309.995    219.623     1225.0    313.802    219.098     1229.0    318.071    221.809     1243.0    321.354    225.726     1252.0    318.365    226.763     1247.0    314.684    227.366     1238.0    310.006    227.600     1234.0    305.399    227.499     1229.0    301.746    226.915     1225.0    301.669    225.023     1225.0    305.318    223.196     1221.0    309.943    222.799     1225.0    314.647    223.204     1234.0    318.542    224.980     1247.0    314.528    223.766     1234.0    309.949    223.478     1225.0    305.449    223.771     1221.0           281.095           203.078            1243.0           280.479           209.879            1252.0           280.602           216.520            1256.0   \n",
            "13734    1247.0   309.199   258.364    1252.0   309.604   253.476    1252.0   305.702   249.036  1252.000000   303.868   243.124  1256.000000   302.892   236.992  1256.000000    291.757    270.911     1243.0    294.718    268.918     1243.0    298.346    267.668     1238.0    301.995    268.253     1238.0    305.498    267.200     1243.0    310.304    267.779     1256.0    315.488    269.087     1031.0    312.197    273.428     1022.0    308.214    276.249     1019.0    303.121    277.478     1019.0    298.374    276.750     1022.0    294.908    274.507     1243.0    293.659    271.244     1243.0    297.590    270.552     1243.0    302.138    270.522     1238.0    307.497    269.801     1034.0    313.338    269.795     1031.0    308.067    272.395     1025.0    302.503    273.491     1028.0    297.743    272.947     1243.0           272.069           238.906            1280.0           272.517           247.159            1280.0           274.053           254.858            1275.0   \n",
            "\n",
            "       71x_face_contour  71y_face_contour  71z_face_contour  72x_face_contour  72y_face_contour  72z_face_contour  73x_face_contour  73y_face_contour  73z_face_contour  74x_face_contour  74y_face_contour  74z_face_contour  75x_face_contour  75y_face_contour  75z_face_contour  76x_face_contour  76y_face_contour  76z_face_contour  77x_face_contour  77y_face_contour  77z_face_contour  78x_face_contour  78y_face_contour  78z_face_contour  79x_face_contour  79y_face_contour  79z_face_contour  80x_face_contour  80y_face_contour  80z_face_contour  81x_face_contour  81y_face_contour  81z_face_contour  82x_face_contour  82y_face_contour  82z_face_contour  83x_face_contour  83y_face_contour  83z_face_contour  84x_face_contour  84y_face_contour  84z_face_contour  85x_face_contour  85y_face_contour  85z_face_contour  86x_face_contour  86y_face_contour  86z_face_contour  87x_left_iris  87y_left_iris  87z_left_iris  88x_right_iris  88y_right_iris  88z_right_iris  89x_nose_tip  89y_nose_tip  \\\n",
            "3366            294.627           255.217            1221.0           296.617           263.679            1212.0           299.512           271.495            1204.0           302.638           278.363            1212.0           306.146           284.688            1221.0           310.602           289.204            1270.0           316.890           291.541            1243.0           325.009           290.875            1243.0           333.032           287.839            1247.0           340.209           282.895       1253.333333           346.269           277.033       1270.000000           351.254           268.904       1258.444444           354.218           260.073       1252.481481           355.669           250.966       1252.000000           355.858           241.892            1247.0           355.736           232.819            1243.0        306.322        230.213         1200.0         331.568         232.829          1212.0       314.864       251.520   \n",
            "6885            268.560           223.692            1234.0           270.372           229.300            1234.0           272.862           234.350            1243.0           275.764           238.284            1247.0           279.352           241.852            1247.0           284.357           244.908            1247.0           291.099           246.059            1256.0           298.572           245.281            1265.0           305.527           242.862            1275.0           311.449           239.163       1294.000000           316.009           234.434       1309.000000           319.142           228.977       1515.000000           320.863           222.853       1536.000000           321.926           216.507       1453.333333           322.713           209.968            1309.0           322.769           202.743            1304.0        275.814        203.091         1221.0         304.633         201.729          1238.0       289.144       213.955   \n",
            "27397           312.035           241.806            1261.0           314.335           247.209            1252.0           317.859           252.148            1243.0           322.053           256.066            1238.0           326.927           259.154            1238.0           332.492           261.172            1234.0           338.750           261.811            1238.0           345.341           260.926            1247.0           351.187           258.719            1256.0           356.286           255.325       1261.000000           360.550           251.198       1267.000000           363.811           246.029       1284.000000           365.826           240.291       1289.000000           366.858           234.375       1299.000000           367.180           228.388            1304.0           366.873           222.791            1304.0        324.950        223.724         1270.0         350.921         223.094          1275.0       338.104       235.803   \n",
            "19636           281.336           223.287            1261.0           282.859           229.874            1294.0           285.652           235.701            1294.0           289.843           239.996            1284.0           295.236           243.435            1280.0           301.924           245.871            1275.0           309.856           246.898            1280.0           317.899           246.021            1294.0           325.197           243.749            1304.0           331.242           239.950       1319.000000           335.806           235.014       1529.000000           338.646           229.021       1536.000000           340.191           222.522       1549.000000           341.136           215.911       1556.000000           341.387           209.484            1334.0           340.984           202.722            1294.0        296.388        199.841         1225.0         325.509         199.975          1238.0       310.227       211.028   \n",
            "13734           276.699           262.276            1265.0           279.946           269.215            1256.0           283.926           275.224            1247.0           288.176           280.633            1252.0           292.945           285.647            1016.0           298.308           289.387            1001.0           304.841           290.806             998.0           311.918           289.339             998.0           318.260           285.539            1001.0           323.756           280.284       1016.000000           328.456           274.337       1198.666667           332.344           267.259       1549.000000           334.620           259.420       1294.000000           335.512           251.058       1294.000000           335.549           242.599            1289.0           334.863           233.911            1289.0        284.207        237.583         1256.0         314.169         235.561          1261.0       300.121       255.670   \n",
            "\n",
            "       89z_nose_tip  90x_line_above_left_eyebrow  90y_line_above_left_eyebrow  90z_line_above_left_eyebrow  91x_line_above_left_eyebrow  91y_line_above_left_eyebrow  91z_line_above_left_eyebrow  92x_line_above_left_eyebrow  92y_line_above_left_eyebrow  92z_line_above_left_eyebrow  93x_line_above_left_eyebrow  93y_line_above_left_eyebrow  93z_line_above_left_eyebrow  94x_line_above_left_eyebrow  94y_line_above_left_eyebrow  94z_line_above_left_eyebrow  95x_line_above_right_eyebrow  95y_line_above_right_eyebrow  95z_line_above_right_eyebrow  96x_line_above_right_eyebrow  96y_line_above_right_eyebrow  96z_line_above_right_eyebrow  97x_line_above_right_eyebrow  97y_line_above_right_eyebrow  97z_line_above_right_eyebrow  98x_line_above_right_eyebrow  98y_line_above_right_eyebrow  98z_line_above_right_eyebrow  99x_line_above_right_eyebrow  99y_line_above_right_eyebrow  99z_line_above_right_eyebrow  left_eyebrow_length  right_eyebrow_length  mouth_length  left_eye_width  right_eye_width  \\\n",
            "3366         1204.0                      314.106                      219.253                       1221.0                      310.861                      216.805                       1221.0                      306.403                      215.156                       1225.0                      301.975                      215.488                       1234.0                      298.952                      217.402                       1238.0                       323.465                       219.163                        1221.0                       328.400                       217.794                        1212.0                       334.567                       217.487                        1225.0                       339.792                       219.232                        1221.0                       343.365                       221.709                        1225.0            29.747687             25.914962     19.753773        9.969030        11.720913   \n",
            "6885         1221.0                      281.365                      189.995                       1212.0                      276.218                      189.072                       1208.0                      269.917                      189.561                       1243.0                      265.494                      192.432                       1243.0                      263.230                      195.746                       1243.0                       295.863                       189.261                        1238.0                       301.540                       187.721                        1247.0                       308.751                       187.235                        1256.0                       314.311                       189.517                        1270.0                       317.662                       192.398                        1284.0            37.859572             38.254211     33.984588       13.728226        26.079997   \n",
            "27397        1238.0                      331.939                      214.602                       1247.0                      328.282                      213.494                       1256.0                      324.247                      212.960                       1284.0                      319.988                      213.197                       1294.0                      316.592                      214.298                       1294.0                       343.585                       214.272                        1289.0                       347.058                       213.010                        1294.0                       351.240                       212.224                        1289.0                       355.611                       212.154                        1289.0                       359.132                       212.980                        1304.0            44.644828             19.100412     23.519855       21.470832        13.408294   \n",
            "19636        1225.0                      304.142                      186.400                       1221.0                      298.171                      185.324                       1225.0                      290.908                      185.868                       1247.0                      285.694                      188.956                       1247.0                      282.774                      192.646                       1247.0                       317.743                       186.305                        1252.0                       323.778                       185.360                        1252.0                       331.382                       186.016                        1265.0                       336.702                       189.209                        1275.0                       339.453                       192.716                        1284.0            38.499209             37.657358     35.130215       17.558107        24.984519   \n",
            "13734        1247.0                      290.428                      223.245                       1252.0                      285.600                      221.512                       1270.0                      279.228                      220.851                       1280.0                      274.033                      222.983                       1284.0                      271.144                      226.133                       1299.0                       304.363                       221.731                        1270.0                       309.456                       219.509                        1265.0                       316.046                       217.997                        1275.0                       321.838                       219.217                        1275.0                       325.661                       221.533                        1284.0            48.107060             30.429217    213.331871       25.982603        15.546276   \n",
            "\n",
            "       nose_to_left_eye  nose_to_right_eye  nose_to_mouth_center  left_eyebrow_angle  right_eyebrow_angle  nose_to_eyes_angle  mouth_angle  \n",
            "3366          20.617014          22.759719             17.839867          143.800797           144.971364           51.050770   122.848680  \n",
            "6885          13.489104          17.152775             14.602698          135.282412           158.389183           74.281107   109.928801  \n",
            "27397         23.126374          39.781349             12.495552          166.486321           162.981203           35.397289   117.398751  \n",
            "19636         13.483894          16.913600             13.728806          106.481485           153.992060           78.468545   103.451115  \n",
            "13734         20.311633          22.454522            122.321837          149.956690           144.572513           54.506212   118.846230  \n"
          ]
        }
      ],
      "source": [
        "print(X_test_added.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking the impact of new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_columns = [\n",
        "    \"left_eyebrow_length\",\n",
        "    \"right_eyebrow_length\",\n",
        "    \"mouth_length\",\n",
        "    \"left_eye_width\",\n",
        "    \"right_eye_width\",\n",
        "    \"nose_to_left_eye\",\n",
        "    \"nose_to_right_eye\",\n",
        "    \"nose_to_mouth_center\",\n",
        "    \"left_eyebrow_angle\",\n",
        "    \"right_eyebrow_angle\",\n",
        "    \"nose_to_eyes_angle\",\n",
        "    \"mouth_angle\"\n",
        "]\n",
        "\n",
        "\n",
        "X_new_features = X_added[new_columns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_added, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Feature  Importance\n",
            "5       nose_to_left_eye    0.116420\n",
            "0    left_eyebrow_length    0.104753\n",
            "6      nose_to_right_eye    0.102294\n",
            "2           mouth_length    0.090284\n",
            "3         left_eye_width    0.089530\n",
            "10    nose_to_eyes_angle    0.086319\n",
            "7   nose_to_mouth_center    0.081780\n",
            "1   right_eyebrow_length    0.075902\n",
            "4        right_eye_width    0.071591\n",
            "8     left_eyebrow_angle    0.064779\n",
            "11           mouth_angle    0.058763\n",
            "9    right_eyebrow_angle    0.057584\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Wano cech\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_new_features.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.80\n"
          ]
        }
      ],
      "source": [
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         Feature  Importance\n",
            "224             74z_face_contour    0.011153\n",
            "277  92y_line_above_left_eyebrow    0.009545\n",
            "282  94x_line_above_left_eyebrow    0.008532\n",
            "227             75z_face_contour    0.008273\n",
            "274  91y_line_above_left_eyebrow    0.008163\n",
            "236             78z_face_contour    0.007291\n",
            "305             nose_to_left_eye    0.006946\n",
            "191                    63z_mouth    0.006916\n",
            "154                    51y_mouth    0.006516\n",
            "239             79z_face_contour    0.006504\n",
            "197                    65z_mouth    0.006494\n",
            "69              23x_left_eyebrow    0.006462\n",
            "230             76z_face_contour    0.006438\n",
            "271  90y_line_above_left_eyebrow    0.006415\n",
            "194                    64z_mouth    0.006357\n",
            "58              19y_left_eyebrow    0.006274\n",
            "196                    65y_mouth    0.006247\n",
            "70              23y_left_eyebrow    0.006213\n",
            "276  92x_line_above_left_eyebrow    0.006170\n",
            "188                    62z_mouth    0.006136\n",
            "95             31z_right_eyebrow    0.005823\n",
            "232             77y_face_contour    0.005783\n",
            "306            nose_to_right_eye    0.005733\n",
            "55              18y_left_eyebrow    0.005731\n",
            "52              17y_left_eyebrow    0.005728\n",
            "200                    66z_mouth    0.005725\n",
            "92             30z_right_eyebrow    0.005692\n",
            "60              20x_left_eyebrow    0.005649\n",
            "221             73z_face_contour    0.005645\n",
            "161                    53z_mouth    0.005623\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_added, y)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_added.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance.head(30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.93\n"
          ]
        }
      ],
      "source": [
        "y_pred = rf.predict(X_test_added)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.80      0.92      0.85        83\n",
            "   conditional       0.81      0.89      0.85        95\n",
            "doubt_question       0.94      0.96      0.95       148\n",
            "      emphasis       0.89      0.94      0.92        87\n",
            "      negative       0.91      0.96      0.93       125\n",
            "      relative       0.88      0.93      0.90       147\n",
            "        topics       0.89      0.88      0.88        90\n",
            "   wh_question       0.88      0.92      0.90       114\n",
            "   yn_question       0.84      0.96      0.90       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.88      0.93      0.90      2794\n",
            "  weighted avg       0.94      0.93      0.93      2794\n",
            "\n"
          ]
        }
      ],
      "source": [
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          Feature  Importance\n",
            "224              74z_face_contour    0.009113\n",
            "227              75z_face_contour    0.008859\n",
            "280   93y_line_above_left_eyebrow    0.008531\n",
            "58               19y_left_eyebrow    0.008137\n",
            "197                     65z_mouth    0.007796\n",
            "221              73z_face_contour    0.007722\n",
            "277   92y_line_above_left_eyebrow    0.007672\n",
            "233              77z_face_contour    0.007456\n",
            "154                     51y_mouth    0.007440\n",
            "305              nose_to_left_eye    0.007378\n",
            "279   93x_line_above_left_eyebrow    0.007310\n",
            "161                     53z_mouth    0.007126\n",
            "299  99z_line_above_right_eyebrow    0.006687\n",
            "301          right_eyebrow_length    0.006626\n",
            "70               23y_left_eyebrow    0.006568\n",
            "169                     56y_mouth    0.006322\n",
            "230              76z_face_contour    0.006248\n",
            "196                     65y_mouth    0.006193\n",
            "236              78z_face_contour    0.006161\n",
            "274   91y_line_above_left_eyebrow    0.006149\n",
            "292  97y_line_above_right_eyebrow    0.006081\n",
            "242              80z_face_contour    0.006058\n",
            "54               18x_left_eyebrow    0.006051\n",
            "194                     64z_mouth    0.005925\n",
            "66               22x_left_eyebrow    0.005906\n",
            "282   94x_line_above_left_eyebrow    0.005847\n",
            "175                     58y_mouth    0.005843\n",
            "276   92x_line_above_left_eyebrow    0.005811\n",
            "69               23x_left_eyebrow    0.005806\n",
            "61               20y_left_eyebrow    0.005790\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_added) \n",
        "X_test = scaler.transform(X_test_added)       \n",
        "\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_added.columns,  \n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "\n",
        "print(feature_importance.head(30))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.93\n"
          ]
        }
      ],
      "source": [
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.94      0.95      1788\n",
            "   affirmative       0.85      0.89      0.87        83\n",
            "   conditional       0.83      0.89      0.86        95\n",
            "doubt_question       0.93      0.95      0.94       148\n",
            "      emphasis       0.91      0.92      0.91        87\n",
            "      negative       0.91      0.95      0.93       125\n",
            "      relative       0.89      0.92      0.91       147\n",
            "        topics       0.88      0.86      0.87        90\n",
            "   wh_question       0.87      0.95      0.91       114\n",
            "   yn_question       0.84      0.95      0.89       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.89      0.92      0.90      2794\n",
            "  weighted avg       0.94      0.93      0.94      2794\n",
            "\n"
          ]
        }
      ],
      "source": [
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1/20:\n",
            "Parameters: {'pca__n_components': 0.99, 'rf__n_estimators': 150, 'rf__max_depth': 10, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.7183\n",
            "\n",
            "Iteration 2/20:\n",
            "Parameters: {'pca__n_components': 0.99, 'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_leaf': 1, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.8890\n",
            "\n",
            "Iteration 3/20:\n",
            "Parameters: {'pca__n_components': 0.8, 'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.5601\n",
            "\n",
            "Iteration 4/20:\n",
            "Parameters: {'pca__n_components': 0.95, 'rf__n_estimators': 100, 'rf__max_depth': None, 'rf__min_samples_leaf': 1, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.8747\n",
            "\n",
            "Iteration 5/20:\n",
            "Parameters: {'pca__n_components': 0.8, 'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_leaf': 4, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.6779\n",
            "\n",
            "Iteration 6/20:\n",
            "Parameters: {'pca__n_components': 0.85, 'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.6922\n",
            "\n",
            "Iteration 7/20:\n",
            "Parameters: {'pca__n_components': 0.8, 'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.6807\n",
            "\n",
            "Iteration 8/20:\n",
            "Parameters: {'pca__n_components': 0.99, 'rf__n_estimators': 150, 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.7280\n",
            "\n",
            "Iteration 9/20:\n",
            "Parameters: {'pca__n_components': 0.85, 'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.6850\n",
            "\n",
            "Iteration 10/20:\n",
            "Parameters: {'pca__n_components': 0.8, 'rf__n_estimators': 150, 'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.6951\n",
            "\n",
            "Iteration 11/20:\n",
            "Parameters: {'pca__n_components': 0.99, 'rf__n_estimators': 200, 'rf__max_depth': None, 'rf__min_samples_leaf': 1, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.8873\n",
            "\n",
            "Iteration 12/20:\n",
            "Parameters: {'pca__n_components': 0.85, 'rf__n_estimators': 150, 'rf__max_depth': 30, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.6854\n",
            "\n",
            "Iteration 13/20:\n",
            "Parameters: {'pca__n_components': 0.95, 'rf__n_estimators': 50, 'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.8704\n",
            "\n",
            "Iteration 14/20:\n",
            "Parameters: {'pca__n_components': 0.95, 'rf__n_estimators': 150, 'rf__max_depth': None, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.8593\n",
            "\n",
            "Iteration 15/20:\n",
            "Parameters: {'pca__n_components': 0.95, 'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.8712\n",
            "\n",
            "Iteration 16/20:\n",
            "Parameters: {'pca__n_components': 0.85, 'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_leaf': 4, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.5558\n",
            "\n",
            "Iteration 17/20:\n",
            "Parameters: {'pca__n_components': 0.85, 'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.5709\n",
            "\n",
            "Iteration 18/20:\n",
            "Parameters: {'pca__n_components': 0.99, 'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_leaf': 4, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.8708\n",
            "\n",
            "Iteration 19/20:\n",
            "Parameters: {'pca__n_components': 0.95, 'rf__n_estimators': 150, 'rf__max_depth': None, 'rf__min_samples_leaf': 4, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.8515\n",
            "\n",
            "Iteration 20/20:\n",
            "Parameters: {'pca__n_components': 0.8, 'rf__n_estimators': 50, 'rf__max_depth': None, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.6847\n",
            "\n",
            "\n",
            "Number of models with validation accuracy >= 90%: 0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "seed = 48\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_added)\n",
        "X_val = scaler.transform(X_val_added)\n",
        "X_test = scaler.transform(X_test_added)\n",
        "\n",
        "smote = SMOTE(random_state=seed)\n",
        "X_train, y_train = smote.fit_resample(X_train, y)\n",
        "\n",
        "param_distributions = {\n",
        "    'pca__n_components': [0.8, 0.85, 0.9, 0.95, 0.99],\n",
        "    'rf__n_estimators': [50, 100, 150, 200],\n",
        "    'rf__max_depth': [None, 10, 20, 30],\n",
        "    'rf__min_samples_leaf': [1, 2, 4],\n",
        "    'rf__criterion': ['gini', 'entropy'],\n",
        "}\n",
        "\n",
        "def get_random_params(param_distributions):\n",
        "    return {key: random.choice(values) for key, values in param_distributions.items()}\n",
        "\n",
        "\n",
        "\n",
        "n_iter = 20\n",
        "high_accuracy_models = []\n",
        "\n",
        "for i in range(n_iter):\n",
        "    params = get_random_params(param_distributions)\n",
        "    pca = PCA(n_components=params['pca__n_components'])\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=params['rf__n_estimators'],\n",
        "        max_depth=params['rf__max_depth'],\n",
        "        min_samples_leaf=params['rf__min_samples_leaf'],\n",
        "        criterion=params['rf__criterion'],\n",
        "        random_state=seed,\n",
        "    )\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_val_pca = pca.transform(X_val)\n",
        "    rf.fit(X_train_pca, y_train)\n",
        "    y_val_pred = rf.predict(X_val_pca)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    if val_accuracy >= 0.90:\n",
        "        high_accuracy_models.append((rf, pca, val_accuracy))\n",
        "    print(f\"Iteration {i+1}/{n_iter}:\")\n",
        "    print(f\"Parameters: {params}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nNumber of models with validation accuracy >= 90%: {len(high_accuracy_models)}\")\n",
        "\n",
        "for idx, (model, pca, val_acc) in enumerate(high_accuracy_models, start=1):\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    y_test_pred = model.predict(X_test_pca)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"\\nModel {idx}:\")\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report on Test Set:\")\n",
        "    print(classification_report(y_test, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1/20:\n",
            "Parameters: {'rf__n_estimators': 150, 'rf__max_depth': 10, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.7359\n",
            "\n",
            "Iteration 2/20:\n",
            "Parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_leaf': 1, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.9341\n",
            "\n",
            "Iteration 3/20:\n",
            "Parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_leaf': 4, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.9148\n",
            "\n",
            "Iteration 4/20:\n",
            "Parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.6074\n",
            "\n",
            "Iteration 5/20:\n",
            "Parameters: {'rf__n_estimators': 50, 'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.9266\n",
            "\n",
            "Iteration 6/20:\n",
            "Parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_leaf': 4, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.9159\n",
            "\n",
            "Iteration 7/20:\n",
            "Parameters: {'rf__n_estimators': 100, 'rf__max_depth': None, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.9227\n",
            "\n",
            "Iteration 8/20:\n",
            "Parameters: {'rf__n_estimators': 150, 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.9012\n",
            "\n",
            "Iteration 9/20:\n",
            "Parameters: {'rf__n_estimators': 150, 'rf__max_depth': 20, 'rf__min_samples_leaf': 4, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.8951\n",
            "\n",
            "Iteration 10/20:\n",
            "Parameters: {'rf__n_estimators': 150, 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.7405\n",
            "\n",
            "Iteration 11/20:\n",
            "Parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.7362\n",
            "\n",
            "Iteration 12/20:\n",
            "Parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.9313\n",
            "\n",
            "Iteration 13/20:\n",
            "Parameters: {'rf__n_estimators': 150, 'rf__max_depth': 30, 'rf__min_samples_leaf': 4, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.9173\n",
            "\n",
            "Iteration 14/20:\n",
            "Parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.7294\n",
            "\n",
            "Iteration 15/20:\n",
            "Parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.8987\n",
            "\n",
            "Iteration 16/20:\n",
            "Parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.9295\n",
            "\n",
            "Iteration 17/20:\n",
            "Parameters: {'rf__n_estimators': 150, 'rf__max_depth': None, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.9245\n",
            "\n",
            "Iteration 18/20:\n",
            "Parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_leaf': 4, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.9273\n",
            "\n",
            "Iteration 19/20:\n",
            "Parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__criterion': 'entropy'}\n",
            "Validation Accuracy: 0.9306\n",
            "\n",
            "Iteration 20/20:\n",
            "Parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__criterion': 'gini'}\n",
            "Validation Accuracy: 0.6024\n",
            "\n",
            "\n",
            "Number of models with validation accuracy >= 90%: 12\n",
            "\n",
            "Model 1:\n",
            "Validation Accuracy: 0.9341\n",
            "Test Accuracy: 0.9309\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.78      0.92      0.84        83\n",
            "   conditional       0.80      0.88      0.84        95\n",
            "doubt_question       0.93      0.96      0.95       148\n",
            "      emphasis       0.89      0.92      0.90        87\n",
            "      negative       0.89      0.94      0.92       125\n",
            "      relative       0.90      0.93      0.91       147\n",
            "        topics       0.88      0.87      0.87        90\n",
            "   wh_question       0.88      0.94      0.91       114\n",
            "   yn_question       0.84      0.95      0.89       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.88      0.92      0.90      2794\n",
            "  weighted avg       0.93      0.93      0.93      2794\n",
            "\n",
            "\n",
            "Model 2:\n",
            "Validation Accuracy: 0.9148\n",
            "Test Accuracy: 0.9220\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.92      0.94      1788\n",
            "   affirmative       0.74      0.92      0.82        83\n",
            "   conditional       0.80      0.87      0.83        95\n",
            "doubt_question       0.95      0.97      0.96       148\n",
            "      emphasis       0.86      0.94      0.90        87\n",
            "      negative       0.83      0.97      0.90       125\n",
            "      relative       0.88      0.93      0.90       147\n",
            "        topics       0.87      0.87      0.87        90\n",
            "   wh_question       0.85      0.95      0.90       114\n",
            "   yn_question       0.81      0.95      0.87       117\n",
            "\n",
            "      accuracy                           0.92      2794\n",
            "     macro avg       0.86      0.93      0.89      2794\n",
            "  weighted avg       0.93      0.92      0.92      2794\n",
            "\n",
            "\n",
            "Model 3:\n",
            "Validation Accuracy: 0.9266\n",
            "Test Accuracy: 0.9248\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.77      0.90      0.83        83\n",
            "   conditional       0.79      0.88      0.84        95\n",
            "doubt_question       0.93      0.96      0.94       148\n",
            "      emphasis       0.87      0.91      0.89        87\n",
            "      negative       0.86      0.95      0.90       125\n",
            "      relative       0.89      0.92      0.91       147\n",
            "        topics       0.88      0.86      0.87        90\n",
            "   wh_question       0.86      0.93      0.89       114\n",
            "   yn_question       0.82      0.96      0.88       117\n",
            "\n",
            "      accuracy                           0.92      2794\n",
            "     macro avg       0.86      0.92      0.89      2794\n",
            "  weighted avg       0.93      0.92      0.93      2794\n",
            "\n",
            "\n",
            "Model 4:\n",
            "Validation Accuracy: 0.9159\n",
            "Test Accuracy: 0.9227\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.92      0.94      1788\n",
            "   affirmative       0.75      0.93      0.83        83\n",
            "   conditional       0.78      0.87      0.83        95\n",
            "doubt_question       0.93      0.96      0.95       148\n",
            "      emphasis       0.88      0.93      0.91        87\n",
            "      negative       0.85      0.95      0.90       125\n",
            "      relative       0.88      0.93      0.90       147\n",
            "        topics       0.86      0.88      0.87        90\n",
            "   wh_question       0.86      0.94      0.90       114\n",
            "   yn_question       0.81      0.96      0.88       117\n",
            "\n",
            "      accuracy                           0.92      2794\n",
            "     macro avg       0.86      0.93      0.89      2794\n",
            "  weighted avg       0.93      0.92      0.92      2794\n",
            "\n",
            "\n",
            "Model 5:\n",
            "Validation Accuracy: 0.9227\n",
            "Test Accuracy: 0.9291\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.81      0.92      0.86        83\n",
            "   conditional       0.82      0.88      0.85        95\n",
            "doubt_question       0.93      0.95      0.94       148\n",
            "      emphasis       0.90      0.94      0.92        87\n",
            "      negative       0.88      0.96      0.92       125\n",
            "      relative       0.90      0.93      0.91       147\n",
            "        topics       0.83      0.88      0.85        90\n",
            "   wh_question       0.87      0.94      0.90       114\n",
            "   yn_question       0.81      0.96      0.88       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.87      0.93      0.90      2794\n",
            "  weighted avg       0.93      0.93      0.93      2794\n",
            "\n",
            "\n",
            "Model 6:\n",
            "Validation Accuracy: 0.9012\n",
            "Test Accuracy: 0.9077\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.89      0.93      1788\n",
            "   affirmative       0.62      0.96      0.75        83\n",
            "   conditional       0.75      0.87      0.81        95\n",
            "doubt_question       0.92      0.96      0.94       148\n",
            "      emphasis       0.84      0.93      0.89        87\n",
            "      negative       0.81      0.94      0.87       125\n",
            "      relative       0.86      0.91      0.89       147\n",
            "        topics       0.86      0.90      0.88        90\n",
            "   wh_question       0.86      0.94      0.90       114\n",
            "   yn_question       0.79      0.95      0.86       117\n",
            "\n",
            "      accuracy                           0.91      2794\n",
            "     macro avg       0.83      0.93      0.87      2794\n",
            "  weighted avg       0.92      0.91      0.91      2794\n",
            "\n",
            "\n",
            "Model 7:\n",
            "Validation Accuracy: 0.9313\n",
            "Test Accuracy: 0.9338\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.94      0.95      1788\n",
            "   affirmative       0.75      0.90      0.82        83\n",
            "   conditional       0.83      0.91      0.87        95\n",
            "doubt_question       0.93      0.95      0.94       148\n",
            "      emphasis       0.94      0.92      0.93        87\n",
            "      negative       0.92      0.96      0.94       125\n",
            "      relative       0.92      0.93      0.92       147\n",
            "        topics       0.88      0.89      0.88        90\n",
            "   wh_question       0.87      0.94      0.90       114\n",
            "   yn_question       0.82      0.94      0.88       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.88      0.93      0.90      2794\n",
            "  weighted avg       0.94      0.93      0.93      2794\n",
            "\n",
            "\n",
            "Model 8:\n",
            "Validation Accuracy: 0.9173\n",
            "Test Accuracy: 0.9202\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.91      0.94      1788\n",
            "   affirmative       0.73      0.90      0.81        83\n",
            "   conditional       0.77      0.87      0.82        95\n",
            "doubt_question       0.93      0.96      0.94       148\n",
            "      emphasis       0.87      0.93      0.90        87\n",
            "      negative       0.86      0.96      0.91       125\n",
            "      relative       0.88      0.92      0.90       147\n",
            "        topics       0.86      0.88      0.87        90\n",
            "   wh_question       0.86      0.94      0.90       114\n",
            "   yn_question       0.81      0.97      0.88       117\n",
            "\n",
            "      accuracy                           0.92      2794\n",
            "     macro avg       0.85      0.92      0.89      2794\n",
            "  weighted avg       0.93      0.92      0.92      2794\n",
            "\n",
            "\n",
            "Model 9:\n",
            "Validation Accuracy: 0.9295\n",
            "Test Accuracy: 0.9284\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.75      0.90      0.82        83\n",
            "   conditional       0.82      0.86      0.84        95\n",
            "doubt_question       0.93      0.95      0.94       148\n",
            "      emphasis       0.90      0.93      0.92        87\n",
            "      negative       0.90      0.94      0.92       125\n",
            "      relative       0.89      0.91      0.90       147\n",
            "        topics       0.86      0.86      0.86        90\n",
            "   wh_question       0.87      0.95      0.91       114\n",
            "   yn_question       0.84      0.97      0.90       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.87      0.92      0.90      2794\n",
            "  weighted avg       0.93      0.93      0.93      2794\n",
            "\n",
            "\n",
            "Model 10:\n",
            "Validation Accuracy: 0.9245\n",
            "Test Accuracy: 0.9316\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.81      0.90      0.85        83\n",
            "   conditional       0.83      0.91      0.87        95\n",
            "doubt_question       0.94      0.95      0.95       148\n",
            "      emphasis       0.90      0.93      0.92        87\n",
            "      negative       0.88      0.98      0.92       125\n",
            "      relative       0.90      0.93      0.91       147\n",
            "        topics       0.85      0.88      0.86        90\n",
            "   wh_question       0.87      0.95      0.91       114\n",
            "   yn_question       0.81      0.96      0.88       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.88      0.93      0.90      2794\n",
            "  weighted avg       0.94      0.93      0.93      2794\n",
            "\n",
            "\n",
            "Model 11:\n",
            "Validation Accuracy: 0.9273\n",
            "Test Accuracy: 0.9298\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.82      0.92      0.86        83\n",
            "   conditional       0.82      0.88      0.85        95\n",
            "doubt_question       0.93      0.95      0.94       148\n",
            "      emphasis       0.89      0.93      0.91        87\n",
            "      negative       0.87      0.98      0.92       125\n",
            "      relative       0.90      0.93      0.91       147\n",
            "        topics       0.85      0.88      0.86        90\n",
            "   wh_question       0.86      0.95      0.90       114\n",
            "   yn_question       0.81      0.96      0.88       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.87      0.93      0.90      2794\n",
            "  weighted avg       0.93      0.93      0.93      2794\n",
            "\n",
            "\n",
            "Model 12:\n",
            "Validation Accuracy: 0.9306\n",
            "Test Accuracy: 0.9313\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.93      0.95      1788\n",
            "   affirmative       0.77      0.93      0.84        83\n",
            "   conditional       0.82      0.89      0.85        95\n",
            "doubt_question       0.94      0.95      0.95       148\n",
            "      emphasis       0.92      0.92      0.92        87\n",
            "      negative       0.90      0.95      0.93       125\n",
            "      relative       0.90      0.93      0.91       147\n",
            "        topics       0.87      0.87      0.87        90\n",
            "   wh_question       0.87      0.94      0.90       114\n",
            "   yn_question       0.82      0.95      0.88       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.88      0.93      0.90      2794\n",
            "  weighted avg       0.93      0.93      0.93      2794\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "seed = 48\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_added)\n",
        "X_val = scaler.transform(X_val_added)\n",
        "X_test = scaler.transform(X_test_added)\n",
        "\n",
        "smote = SMOTE(random_state=seed)\n",
        "X_train, y_train = smote.fit_resample(X_train, y)\n",
        "\n",
        "param_distributions = {\n",
        "    'rf__n_estimators': [50, 100, 150, 200],\n",
        "    'rf__max_depth': [None, 10, 20, 30],\n",
        "    'rf__min_samples_leaf': [1, 2, 4],\n",
        "    'rf__criterion': ['gini', 'entropy'],\n",
        "}\n",
        "\n",
        "def get_random_params(param_distributions):\n",
        "    return {key: random.choice(values) for key, values in param_distributions.items()}\n",
        "\n",
        "\n",
        "n_iter = 20\n",
        "high_accuracy_models = []\n",
        "\n",
        "for i in range(n_iter):\n",
        "    params = get_random_params(param_distributions)\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=params['rf__n_estimators'],\n",
        "        max_depth=params['rf__max_depth'],\n",
        "        min_samples_leaf=params['rf__min_samples_leaf'],\n",
        "        criterion=params['rf__criterion'],\n",
        "        random_state=seed,\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_val_pred = rf.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    if val_accuracy >= 0.90:\n",
        "        high_accuracy_models.append((rf, val_accuracy))\n",
        "    print(f\"Iteration {i+1}/{n_iter}:\")\n",
        "    print(f\"Parameters: {params}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nNumber of models with validation accuracy >= 90%: {len(high_accuracy_models)}\")\n",
        "\n",
        "for idx, (model, val_acc) in enumerate(high_accuracy_models, start=1):\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"\\nModel {idx}:\")\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report on Test Set:\")\n",
        "    print(classification_report(y_test, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_added) \n",
        "X_test = scaler.transform(X_test_added)       \n",
        "\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9349\n"
          ]
        }
      ],
      "source": [
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          None       0.97      0.94      0.95      1788\n",
            "   affirmative       0.85      0.89      0.87        83\n",
            "   conditional       0.83      0.89      0.86        95\n",
            "doubt_question       0.93      0.95      0.94       148\n",
            "      emphasis       0.91      0.92      0.91        87\n",
            "      negative       0.91      0.95      0.93       125\n",
            "      relative       0.89      0.92      0.91       147\n",
            "        topics       0.88      0.86      0.87        90\n",
            "   wh_question       0.87      0.95      0.91       114\n",
            "   yn_question       0.84      0.95      0.89       117\n",
            "\n",
            "      accuracy                           0.93      2794\n",
            "     macro avg       0.89      0.92      0.90      2794\n",
            "  weighted avg       0.94      0.93      0.94      2794\n",
            "\n"
          ]
        }
      ],
      "source": [
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'class_weight': None,\n",
              " 'criterion': 'gini',\n",
              " 'max_depth': None,\n",
              " 'max_features': 'sqrt',\n",
              " 'max_leaf_nodes': None,\n",
              " 'max_samples': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'monotonic_cst': None,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': None,\n",
              " 'oob_score': False,\n",
              " 'random_state': 42,\n",
              " 'verbose': 0,\n",
              " 'warm_start': False}"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rf.get_params()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pycaret-venv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
